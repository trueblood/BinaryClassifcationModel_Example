{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a183c9-100f-423c-859a-726fa687c1d2",
   "metadata": {},
   "source": [
    "Project Name:\n",
    "Fall Detection with Perceptron\n",
    "\n",
    "Filename:\n",
    "fall_detection_perceptron.py\n",
    "\n",
    "Short Description:\n",
    "This project uses a Perceptron model to detect falls based on incident descriptions. The text data is vectorized using TF-IDF and then classified as a fall or non-fall using the perceptron learning rule. The model can predict fall events based on incident text, offering a simple yet effective method for classification.\n",
    "\n",
    "Perceptron Learning Rule:\n",
    "The perceptron learning rule updates the model's weights incrementally based on the error in prediction. If the perceptron makes an incorrect prediction, the weights are adjusted by adding the product of the learning rate, the input vector, and the difference between the actual and predicted output. This process continues until the model converges to a solution or reaches the maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2bbf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/squeebit/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/squeebit/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/squeebit/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/squeebit/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/squeebit/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /Users/squeebit/Library/Python/3.9/lib/python/site-packages (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/squeebit/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/squeebit/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/squeebit/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/squeebit/Library/Python/3.9/lib/python/site-packages (from scikit-learn) (2.0.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas\n",
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ecf9e",
   "metadata": {},
   "source": [
    "### Clean and Normalize The Data\n",
    "\n",
    "#### Normalizing Text\n",
    "- Converting text to lowercase: Ensures that \"Fall\" and \"fall\" are treated as the same word.\n",
    "- Removing special characters or numbers (optional): Depending on the data, you might want to remove punctuation, numbers, or other irrelevant characters.\n",
    "- Removing stopwords (optional): You might also want to remove common stopwords (like \"and,\" \"the,\" etc.) to reduce noise in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5b305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/squeebit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re # regular expression library\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "class DataCleanerAndNormalizer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def clean_data(self):\n",
    "  #      print(\"First few rows of the dataset:\")\n",
    "  #      print(self.df.head())\n",
    "\n",
    "        # Get a count of rows with missing data\n",
    "        missing_data_count = self.df.isnull().sum()  # the isnull().sum() will show the data type of the counts\n",
    "        print(\"\\nCount of missing data in each column:\")\n",
    "        print(missing_data_count)\n",
    "\n",
    "        # Get the total number of rows with any missing data\n",
    "        rows_with_missing_data = self.df.isnull().any(axis=1).sum()\n",
    "        print(f\"\\nTotal number of rows with missing data: {rows_with_missing_data}\")\n",
    "\n",
    "        # Drop rows with any missing data\n",
    "        df_cleaned = self.df.dropna()\n",
    "\n",
    "        # Display the first few rows of the cleaned DataFrame to verify looks good high level overview\n",
    "        print(\"\\nFirst few rows of the cleaned dataset:\")\n",
    "        print(df_cleaned.head())\n",
    "\n",
    "        return df_cleaned\n",
    "    \n",
    "    def normalize_data(self):\n",
    "        # Normalize text: convert to lowercase and remove special characters\n",
    "        self.df['Description'] = self.df['Description'].apply(lambda x: x.lower() if isinstance(x, str) else x)  # Convert to lowercase for qualatative data\n",
    "       \n",
    "        # self.df['Description'] = self.df['Description'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))  # Remove special characters, optional step\n",
    "        # Removing stopwords (optional): You might also want to remove common stopwords (like \"and,\" \"the,\" etc.) to reduce noise in the dataset.\n",
    "        # For this, you can use the NLTK library.\n",
    "  \n",
    "        stop = stopwords.words('english')\n",
    "        #self.df['Description'] = self.df['Description'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]) if isinstance(x, str) else x)\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def get_most_frequent_words(self, corpus, top_n=10):\n",
    "        combined_text = ' '.join(corpus)\n",
    "        \n",
    "        # Remove numbers\n",
    "        combined_text = re.sub(r'\\d+', '', combined_text)\n",
    "        \n",
    "        # Tokenize the text\n",
    "        words = combined_text.split()\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in stop]\n",
    "        \n",
    "        # Count word frequencies\n",
    "        word_counts = Counter(words)\n",
    "        \n",
    "        # Get the most common words\n",
    "        most_common_words = word_counts.most_common(top_n)\n",
    "        \n",
    "        # Return only the words\n",
    "        return [word for word, count in most_common_words]\n",
    "    \n",
    "    def get_number_of_words_feature_names(self, corpus, max_features=None):\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "        vectorizer.fit(corpus)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        print(\"Unique words in the corpus:\", feature_names)\n",
    "        \n",
    "        return len(feature_names), feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6beabd",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "323fc7ed-a895-4a75-8a06-846d87693536",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count of missing data in each column:\n",
      "Description    2\n",
      "isFallFlg      0\n",
      "dtype: int64\n",
      "\n",
      "Total number of rows with missing data: 2\n",
      "\n",
      "First few rows of the cleaned dataset:\n",
      "                                         Description  isFallFlg\n",
      "0                    Testing an incident submission.          1\n",
      "1                                               test          0\n",
      "2           Melanie fell from her bed this morning.           1\n",
      "3  Pam was found on the floor of her unit; reside...          1\n",
      "4                                              Test3          1\n",
      "\n",
      "Dataset after data cleaning: 16967 rows, 2 columns\n",
      "\n",
      "Dataset after data normalization: 16967 rows, 2 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/74/_jf5jntd0m73y8b26bl9bz780000gn/T/ipykernel_11076/3553120068.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df['Description'] = self.df['Description'].apply(lambda x: x.lower() if isinstance(x, str) else x)  # Convert to lowercase for qualatative data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in the corpus: ['00' '000' '0000' ... 'zygoma' 'zygomatic' 'zyrtec']\n",
      "Number of unique words in the corpus: (17139, array(['00', '000', '0000', ..., 'zygoma', 'zygomatic', 'zyrtec'],\n",
      "      dtype=object))\n",
      "Most frequent words: ['resident', 'floor', 'stated', 'staff', 'called', 'notified', 'found', 'room', '.', 'pain']\n",
      "None\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.83      0.75      1090\n",
      "           1       0.91      0.82      0.86      2304\n",
      "\n",
      "    accuracy                           0.82      3394\n",
      "   macro avg       0.80      0.83      0.81      3394\n",
      "weighted avg       0.84      0.82      0.83      3394\n",
      "\n",
      "Prediction for 'This is a fall': [1]\n",
      "Prediction for 'This is not a fall': [1]\n",
      "Prediction for 'sky': [0]\n",
      "Prediction for 'I flew to the mall': [0]\n",
      "Prediction for 'I slipped in mud': [1]\n",
      "Prediction for 'she fell but not really this is a test': [1]\n",
      "Prediction for 'I walked to the store': [0]\n",
      "Prediction for 'The weather is nice today': [0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the dataset\n",
    "fall_data = pd.read_csv(\"data/fall_data_v1.csv\")\n",
    "\n",
    "# Clean the dataset\n",
    "data_clearner_normalizer = DataCleanerAndNormalizer(fall_data)\n",
    "df_cleaned = data_clearner_normalizer.clean_data()  # Returns the cleaned DataFrame\n",
    "data_clearner_normalizer.df = df_cleaned  # Update the DataFrame for normalization\n",
    "\n",
    "# Call the normalize_data method to normalize the dataset\n",
    "df_normalized = data_clearner_normalizer.normalize_data()\n",
    "\n",
    "# Check if the DataFrame was successfully after data cleaning\n",
    "print(f\"\\nDataset after data cleaning: {df_cleaned.shape[0]} rows, {df_cleaned.shape[1]} columns\")\n",
    "\n",
    "# Check if the DataFrame was successfully after data normalization\n",
    "print(f\"\\nDataset after data normalization: {df_normalized.shape[0]} rows, {df_normalized.shape[1]} columns\")\n",
    "\n",
    "# Get the number of unique words in the corpus\n",
    "corpus = df_cleaned['Description']  # Use the cleaned data\n",
    "num_words = data_clearner_normalizer.get_number_of_words_feature_names(corpus)\n",
    "print(f\"Number of unique words in the corpus: {num_words}\")\n",
    "\n",
    "# Example usage\n",
    "corpus = df_cleaned['Description']  # Use the cleaned data\n",
    "top_words = data_clearner_normalizer.get_most_frequent_words(corpus, top_n=10)\n",
    "print(f\"Most frequent words: {top_words}\")\n",
    "\n",
    "# Extract features and target from the cleaned dataset\n",
    "X_cleaned = df_normalized['Description']\n",
    "y_cleaned = df_normalized['isFallFlg']\n",
    "\n",
    "# Vectorize the cleaned text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "X_vectorized_cleaned = vectorizer.fit_transform(X_cleaned)\n",
    "\n",
    "print(vectorizer.vocabulary)\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_vectorized_cleaned, y_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the perceptron model\n",
    "perceptron_model = Perceptron(max_iter=1000, random_state=42)\n",
    "perceptron_model.fit(X_train_cleaned, y_train_cleaned)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_cleaned = perceptron_model.predict(X_test_cleaned)\n",
    "\n",
    "# Generate and display the classification report\n",
    "report_cleaned = classification_report(y_test_cleaned, y_pred_cleaned)\n",
    "print(\"Classification Report:\\n\", report_cleaned)\n",
    "\n",
    "# Test with sample inputs\n",
    "def predict_fall(description):\n",
    "    test_input_vectorized = vectorizer.transform([description])\n",
    "    prediction = perceptron_model.predict(test_input_vectorized)\n",
    "    return prediction\n",
    "\n",
    "# Test with various inputs\n",
    "print(\"Prediction for 'This is a fall':\", predict_fall(\"This is a fall\"))\n",
    "print(\"Prediction for 'This is not a fall':\", predict_fall(\"This is not a fall\"))\n",
    "print(\"Prediction for 'sky':\", predict_fall(\"sky\"))\n",
    "print(\"Prediction for 'fall':\", predict_fall(\"fall\"))\n",
    "print(\"Prediction for 'I flew to the mall':\", predict_fall(\"I flew to the mall\"))\n",
    "print(\"Prediction for 'I slipped in mud':\", predict_fall(\"I slipped in mud\"))\n",
    "print(\"Prediction for 'she fell but not really this is a test':\", predict_fall(\"she fell but not really this is a test\"))\n",
    "print(\"Prediction for 'I walked to the store':\", predict_fall(\"I walked to the store\"))\n",
    "print(\"Prediction for 'The weather is nice today':\", predict_fall(\"The weather is nice today\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb5ffe0-e64f-4d4b-ac3c-899e19731cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m122",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m122"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
